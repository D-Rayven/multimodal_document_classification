{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Deep Learning: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. La Classification consiste à :\n",
    "- charger les données\n",
    "- les transmettre à travers le modèle\n",
    "- générer une sortie\n",
    "- calculer la perte, \n",
    "- prendre des gradients par rapport aux poids et mettre à jour le modèle. \n",
    "#### Cependant, la forme précise des cibles, le paramétrage de la couche de sortie et le choix de la fonction de perte s'adapteront au réglage de la classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dans cette première partie, nous nous concentrons sur les problèmes de classification où on va plutôt se concentrer sur la question suivante: quelle catégorie appartient cet objet/image ?.\n",
    "#### Exemple de questions:\n",
    "- Cet e-mail appartient-il au dossier spam ou à la boîte de réception ?\n",
    "- Ce client est-il plus susceptible de souscrire ou non à un service d'abonnement ?\n",
    "- Cette image représente-t-elle un âne, un chien, un chat ou un coq ?\n",
    "- Quel film Aston est-il le plus susceptible de regarder ensuite ?\n",
    "- Quelle section du livre allez-vous lire ensuite?\n",
    "\n",
    "\n",
    "- Pour se mouiller les pieds, commençons par un simple problème de classification d'images. Ici, chaque entrée consiste en une image $2\\times2$ en niveaux de gris. Nous pouvons représenter chaque valeur de pixel avec un seul scalaire, nous donnant quatre caractéristiques $x_1, x_2, x_3, x_4$. De plus, supposons que chaque image appartient à l'une des catégories \"chat\", \"poulet\" et \"chien\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensuite, nous devons choisir comment représenter les étiquettes. Nous avons deux choix évidents. L'impulsion la plus naturelle serait peut-être de choisir $y \\in \\{1, 2, 3\\}$, où les nombres entiers représentent $\\{\\text{dog}, \\text{cat}, \\text{chicken}\\}$ respectivement. C'est un excellent moyen de stocker ces informations sur un ordinateur. Si les catégories avaient un ordre naturel entre elles, disons si nous essayions de prédire $\\{\\text{baby}, \\text{toddler}, \\text{adolescent}, \\text{young adult}, \\text{adult}, \\text{geriatric}\\}$, alors il pourrait même être judicieux de le présenter comme un problème de régression ordinale [ordinal regression](https://en.wikipedia.org/wiki/Ordinal_regression) et de conserver les étiquettes dans ce format. Voir Moon et al. ( 2010 ) :citet:`Moon.Smola.Chang.ea.2010` pour un aperçu des différents types de fonctions de perte de classement et Beutel et al. ( 2014 ) :citet:`Beutel.Murray.Faloutsos.ea.2014` pour une approche bayésienne qui aborde les réponses avec plus d'un mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En général, les problèmes de classification ne s'accompagnent pas d'ordres naturels entre les classes. Heureusement, les statisticiens ont depuis longtemps inventé un moyen simple de représenter les données catégorielles : le codage à chaud . Un encodage $\\{\\text{one-hot}\\}$ est un vecteur avec autant de composants que nous avons de catégories. Le composant correspondant à la catégorie d'une instance particulière est défini sur 1 et tous les autres composants sont définis sur 0. Dans notre cas, une étiquette $y$ serait un vecteur tridimensionnel, avec $(1, 0, 0)$ correspondant à « chat », $(0, 1, 0)$ au \"poulet\", et $(0, 0, 1)$ au chien\":\n",
    "\n",
    "$$y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modèle Linéaire.\n",
    "- Afin d'estimer les probabilités conditionnelles associées à toutes les classes possibles, nous avons besoin d'un modèle à sorties multiples, une par classe. Pour aborder la classification avec des modèles linéaires, nous aurons besoin d'autant de fonctions affines que nous avons de sorties. À proprement parler, nous n'en avons besoin que d'un de moins, puisque la dernière catégorie doit être la différence entre $1$ et la somme des autres catégories mais pour des raisons de symétrie nous utilisons une paramétrisation légèrement redondante. Chaque sortie correspond à sa propre fonction affine. Dans notre cas, puisque nous avons 4 caractéristiques et 3 catégories de sortie possibles, nous avons besoin de 12 scalaires pour représenter les poids ($w$ avec des indices), et 3 scalaires pour représenter les biais ($b$ avec des indices). Cela donne :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n",
    "o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n",
    "o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Tout comme dans la régression linéaire, nous utilisons un réseau de neurones à une seule couche. Et depuis le calcul de chaque sortie,  $o_1, o_2$, et $o_3$, dépend de toutes les entrées $x_1$, $x_2$, $x_3$, et $x_4$, la couche de sortie peut également être décrite comme une couche entièrement connectée.\n",
    "\n",
    "- Pour une notation plus concise, nous utilisons des vecteurs et des matrices $\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$: est beaucoup mieux adapté aux mathématiques et au code. Notez que nous avons rassemblé tous nos poids dans une $3 \\times 4$ matrice et tous les biais $\\mathbf{b} \\in \\mathbb{R}^3$ dans un vecteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Le Softmax\n",
    "- En supposant une fonction de perte appropriée, nous pourrions essayer, directement, de minimiser la différence entre $\\mathbf{o}$ et les étiquettes $\\mathbf{y}$. S'il s'avère que le traitement de la classification comme un problème de régression à valeur vectorielle fonctionne étonnamment bien, il manque néanmoins les éléments suivants :\n",
    "\n",
    "    * Il n'y a aucune garantie que les sorties $o_i$ somme jusqu'à $1$ dans la façon dont nous nous attendons à ce que les probabilités se comportent.\n",
    "\n",
    "    * Il n'y a aucune garantie que les sorties $o_i$ sont même non négatifs, même si leurs sorties totalisent $1$, ou qu'ils ne dépassent pas $1$.\n",
    "\n",
    "- Les deux aspects rendent le problème d'estimation difficile à résoudre et la solution très fragile aux valeurs aberrantes. Par exemple, si nous supposons qu'il existe une dépendance linéaire positive entre le nombre de chambres et la probabilité que quelqu'un achète une maison, la probabilité pourrait dépasser $1$ quand il s'agit d'acheter un manoir! En tant que tel, nous avons besoin d'un mécanisme pour \"écraser\" les sorties.\n",
    "\n",
    "- Il existe de nombreuses façons d'atteindre cet objectif. Par exemple, nous pourrions supposer que les sorties $o_i$ sont des versions corrompues de $y$, où la corruption se produit au moyen de l'ajout de bruit $\\mathbf{\\epsilon}$ tirée d'une distribution normale. Autrement dit, $\\mathbf{y} = \\mathbf{o} + \\mathbf{\\epsilon}$, où $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. C'est ce qu'on appelle le modèle probit [probit model](https://en.wikipedia.org/wiki/Probit_model), introduit pour la première fois par Fechner ( 1860 ) :citet:`Fechner.1860`. Bien qu'attirant, cela ne fonctionne pas aussi bien ou conduit à un problème d'optimisation particulièrement agréable, par rapport au softmax.\n",
    "\n",
    "- Une autre façon d'atteindre cet objectif (et d'assurer la non-négativité) qui consiste à utiliser une fonction exponentielle $P(y = i) \\propto \\exp o_i$. Cela satisfait en effet l'exigence selon laquelle la probabilité de classe conditionnelle augmente avec l'augmentation $o_i$, elle est monotone et toutes les probabilités sont non négatives. Nous pouvons ensuite transformer ces valeurs pour qu'elles s'additionnent à $1$ en divisant chacun par leur somme. Ce processus est appelé *normalisation* . L'assemblage de ces deux éléments nous donne la fonction *softmax*:\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\text{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}.$$\n",
    "\n",
    "- Notez que la plus grande coordonnée de $\\mathbf{o}$ correspond à la classe la plus probable selon $\\hat{\\mathbf{y}}$. De plus, comme l'opération softmax préserve l'ordre parmi ses arguments, nous n'avons pas besoin de calculer le softmax pour déterminer à quelle classe a été attribuée la probabilité la plus élevée.\n",
    "\n",
    "$$ \\operatorname*{argmax}_j \\hat y_j = \\operatorname*{argmax}_j o_j.$$\n",
    "\n",
    "- L'idée d'un softmax remonte à Gibbs, qui a adapté des idées de la physique ( Gibbs, 1902 ) :cite:`Gibbs.1902`. Datant encore plus loin, Boltzmann, le père de la thermodynamique moderne, a utilisé cette astuce pour modéliser une distribution sur les états d'énergie dans les molécules de gaz. En particulier, il a découvert que la prévalence d'un état d'énergie dans un ensemble thermodynamique, comme les molécules d'un gaz, est proportionnelle à $\\exp(-E/kT)$. Ici, $E$ est l'énergie d'un état, $T$ est la température, et $k$ est la constante de Boltzmann. Lorsque les statisticiens parlent d'augmenter ou de diminuer la « température » d'un système statistique, ils se réfèrent à l'évolution $T$ afin de favoriser des états d'énergie plus ou moins élevés. Selon l'idée de Gibbs, l'énergie équivaut à l'erreur. Les modèles basés sur l'énergie ( Ranzato et al. , 2007 ) utilisent ce point de vue pour décrire les problèmes d'apprentissage en profondeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vectorisation\n",
    "\n",
    "- Pour améliorer l'efficacité des calculs, nous vectorisons les calculs en mini-lots de données. Supposons qu'on nous donne un mini-lot $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ de $n$ caractéristiques avec dimensionnalité (nombre d'entrées) $d$. De plus, supposons que nous ayons $q$ catégories dans la sortie. Alors les poids satisfont $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$ et le biais satisfait $\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$.\n",
    "\n",
    "$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \n",
    "\\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$\n",
    "\n",
    "- Cela accélère l'opération dominante dans un produit matrice-matrice $\\mathbf{X} \\mathbf{W}$. De plus, étant donné que chaque ligne de $\\mathbf{X}$ représente un exemple de données, l'opération softmax elle-même peut être calculée par *ligne*: pour chaque ligne de $\\mathbf{O}$, exponentiez toutes les entrées, puis normalisez-les par la somme. Notez, cependant, que des précautions doivent être prises pour éviter d'exposer et de prendre des logarithmes de grands nombres, car cela peut provoquer un débordement ou un sous-dépassement numérique. Les frameworks d'apprentissage en profondeur s'en chargent automatiquement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fonction de Perte\n",
    "- Maintenant que nous avons un mappage à partir des fonctionnalités $\\mathbf{x}$ aux probabilités $\\mathbf{\\hat{y}}$, nous avons besoin d'un moyen d'optimiser la précision de cette cartographie. Nous nous appuierons sur l'estimation du maximum de vraisemblance.\n",
    "\n",
    "#### 4.1. Log-Vraisemblance (Log-Likelihood)\n",
    "- La fonction softmax nous donne un vecteur $\\hat{\\mathbf{y}}$, que nous pouvons interpréter comme des probabilités conditionnelles (estimées) de chaque classe, compte tenu de toute entrée $\\mathbf{x}$, tel que $\\hat{y}_1$ = $P(y=\\text{cat} \\mid \\mathbf{x})$. Dans ce qui suit, nous supposons que pour un jeu de données avec des fonctionnalités $\\mathbf{X}$ les étiquettes $\\mathbf{Y}$ sont représentés à l'aide d'un vecteur d'étiquette de codage à chaud (one-hot). Nous pouvons comparer les estimations avec la réalité en vérifiant la probabilité des classes réelles selon notre modèle, compte tenu des caractéristiques :\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n",
    "$$\n",
    "\n",
    "- Nous sommes autorisés à utiliser la factorisation puisque nous supposons que chaque étiquette est tirée indépendamment de sa distribution respective . Étant donné que la maximisation du produit des termes est maladroite, nous prenons le logarithme négatif pour obtenir le problème équivalent de la minimisation de la log-vraisemblance négative :\n",
    "\n",
    "$$\n",
    "-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n",
    "$$\n",
    "\n",
    "- où pour toute paire d'étiquettes $\\mathbf{y}$ et prédiction du modèle $\\hat{\\mathbf{y}}$ pour $q$ classes, la fonction de perte $l$ est:\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$\n",
    "\n",
    "- Pour des raisons expliquées plus loin, la fonction de perte est communément appelée perte d'entropie croisée. Depuis $\\mathbf{y}$ est un vecteur one-hot de longueur $q$, la somme sur toutes ses coordonnées $j$ disparaît pour tous les termes sauf un. A noter que la perte $l(\\mathbf{y}, \\hat{\\mathbf{y}})$ est délimité par le bas par $0$ chaque fois que $\\hat{y}$ est un vecteur de probabilité : aucune entrée n'est plus grande que $1$, donc leur logarithme négatif ne peut pas être inférieur à $0$; $l(\\mathbf{y}, \\hat{\\mathbf{y}}) = 0$ seulement si nous prédisons l'étiquette réelle avec certitude. Cela ne peut jamais arriver pour un réglage fini des poids car prendre une sortie softmax vers $1$ nécessite de prendre l'entrée correspondante $o_i$ à l'infini (ou toutes les autres sorties $o_j$ pour $j \\neq i$ moins l'infini). Même si notre modèle pouvait attribuer une probabilité de sortie de $0$, toute erreur commise lors de l'attribution d'un niveau de confiance aussi élevé entraînerait une perte infinie ($-\\log 0 = \\infty$).\n",
    "\n",
    "#### 4.2. Softmax et perte d'entropie croisée\n",
    "\n",
    "- Étant donné que la fonction softmax et la perte d'entropie croisée correspondante sont si courantes, il est utile de comprendre un peu mieux comment elles sont calculées. En branchant la fonction: $$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\text{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}.$$ dans la définition de la perte en $$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$ et en utilisant la définition du softmax on obtient:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n",
    "&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j \\\\\n",
    "&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Pour comprendre un peu mieux ce qui se passe, considérons la dérivée par rapport à tout logit $o_j$. On a:\n",
    "\n",
    "$$\n",
    "\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n",
    "$$\n",
    "\n",
    "- En d'autres termes, la dérivée est la différence entre la probabilité attribuée par notre modèle, telle qu'exprimée par l'opération softmax, et ce qui s'est réellement passé, tel qu'exprimé par les éléments du vecteur d'étiquette à chaud. En ce sens, cela ressemble beaucoup au problème de la régression, où le gradient est la différence entre l'observation $y$ et la valeur estimée $\\hat{y}$. Ce n'est pas une coïncidence. Dans tout modèle de famille exponentielle, les gradients de la log-vraisemblance sont précisément donnés par ce terme. Ce fait facilite le calcul des gradients dans la pratique.\n",
    "\n",
    "- Considérons maintenant le cas où nous observons non seulement un seul résultat, mais une distribution complète des résultats. On peut utiliser la même représentation que précédemment pour l'étiquette $\\mathbf{y}$. La seule différence est que plutôt qu'un vecteur contenant uniquement des entrées binaires, disons $(0, 0, 1)$, nous avons maintenant un vecteur de probabilité générique, disons $(0.1, 0.2, 0.7)$. Les mathématiques que nous avons utilisées précédemment pour définir la perte $l$: $$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$ fonctionne toujours bien, juste que l'interprétation est légèrement plus générale. C'est la valeur attendue de la perte pour une distribution sur les étiquettes. Cette perte est appelée perte d'entropie croisée et c'est l'une des pertes les plus couramment utilisées pour les problèmes de classification. Nous pouvons démystifier le nom en introduisant simplement les bases de la théorie de l'information. En un mot, il mesure le nombre de bits pour coder ce que nous voyons $y$ par rapport à ce que nous prévoyons $\\hat{\\mathbf{y}}$ qui devrait arriver. Pour plus de détails sur la théorie de l'information, voir ( Cover et Thomas, 1999 ) ou ( MacKay et Mac Kay, 2003 ) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Classification d'images (MNIST) avec l'API Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'un des ensembles de données largement utilisés pour la classification des images est l' ensemble de données MNIST ( LeCun et al. , 1998 ) [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database)  de chiffres manuscrits. Au moment de sa sortie dans les années 1990, il posait un formidable défi à la plupart des algorithmes d'apprentissage automatique, composé de 60 000 images de résolution $28 \\times 28$ pixels en pixels (plus un ensemble de données de test de 10 000 images).\n",
    "\n",
    "- Pendant plus d'une décennie, le MNIST a servi de point de référence pour comparer les algorithmes d'apprentissage automatique. Bien qu'il ait bien fonctionné en tant qu'ensemble de données de référence, même les modèles simples selon les normes actuelles atteignent une précision de classification supérieure à 95 %, ce qui le rend inadapté pour faire la distinction entre les modèles les plus forts et les plus faibles. Plus encore, l'ensemble de données permet des niveaux de précision très élevés, rarement observés dans de nombreux problèmes de classification. Ce développement algorithmique biaisé vers des familles spécifiques d'algorithmes qui peuvent tirer parti d'ensembles de données propres, tels que les méthodes d'ensembles actifs et les algorithmes d'ensembles actifs de recherche de limites. Aujourd'hui, le MNIST sert davantage de vérification de la santé mentale que de référence. ImageNet ( Deng et al. , 2009 )pose un défi beaucoup plus pertinent. Malheureusement, ImageNet est trop volumineux pour la plupart des exemples et illustrations de ce livre, car il faudrait trop de temps pour s'entraîner à rendre les exemples interactifs. Au lieu de cela, nous concentrerons notre discussion dans les sections à venir sur l'ensemble de données Fashion-MNIST qualitativement similaire, mais beaucoup plus petit ( Xiao et al. , 2017 ) , qui a été publié en 2017. Il contient des images de 10 catégories de vêtements à résolution $28 \\times 28$ en pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement du jeu de données¶\n",
    "\n",
    "- Comme il s'agit d'un ensemble de données si fréquemment utilisé, tous les principaux frameworks en fournissent des versions prétraitées. Nous pouvons télécharger et lire l'ensemble de données Fashion-MNIST en mémoire à l'aide de fonctions de cadre intégrées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importer les packages suivants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d2l==1.0.0a1.post0\n",
      "  Using cached d2l-1.0.0a1.post0-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: requests in /home/vincent/anaconda3/lib/python3.9/site-packages (from d2l==1.0.0a1.post0) (2.27.1)\n",
      "Requirement already satisfied: jupyter in /home/vincent/anaconda3/lib/python3.9/site-packages (from d2l==1.0.0a1.post0) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/vincent/anaconda3/lib/python3.9/site-packages (from d2l==1.0.0a1.post0) (3.5.1)\n",
      "Collecting gym\n",
      "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/vincent/anaconda3/lib/python3.9/site-packages (from d2l==1.0.0a1.post0) (1.4.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/vincent/anaconda3/lib/python3.9/site-packages (from d2l==1.0.0a1.post0) (0.1.2)\n",
      "Requirement already satisfied: numpy in /home/vincent/anaconda3/lib/python3.9/site-packages (from d2l==1.0.0a1.post0) (1.21.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from gym->d2l==1.0.0a1.post0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from gym->d2l==1.0.0a1.post0) (4.11.3)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vincent/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0a1.post0) (3.7.0)\n",
      "Requirement already satisfied: nbconvert in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter->d2l==1.0.0a1.post0) (6.4.4)\n",
      "Requirement already satisfied: ipykernel in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter->d2l==1.0.0a1.post0) (6.9.1)\n",
      "Requirement already satisfied: notebook in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter->d2l==1.0.0a1.post0) (6.4.8)\n",
      "Requirement already satisfied: qtconsole in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter->d2l==1.0.0a1.post0) (5.3.0)\n",
      "Requirement already satisfied: jupyter-console in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter->d2l==1.0.0a1.post0) (6.4.0)\n",
      "Requirement already satisfied: ipywidgets in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter->d2l==1.0.0a1.post0) (7.6.5)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (6.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (8.2.0)\n",
      "Requirement already satisfied: nest-asyncio in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (1.5.5)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (6.1.12)\n",
      "Requirement already satisfied: traitlets<6.0,>=5.1.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (5.1.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipykernel->jupyter->d2l==1.0.0a1.post0) (1.5.1)\n",
      "Requirement already satisfied: decorator in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (5.1.1)\n",
      "Requirement already satisfied: backcall in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (61.2.0)\n",
      "Requirement already satisfied: stack-data in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (3.0.20)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (2.11.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (4.9.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/vincent/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (22.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/vincent/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/vincent/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/vincent/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel->jupyter->d2l==1.0.0a1.post0) (1.16.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (5.3.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from ipywidgets->jupyter->d2l==1.0.0a1.post0) (3.5.2)\n",
      "Requirement already satisfied: fastjsonschema in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==1.0.0a1.post0) (2.15.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==1.0.0a1.post0) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->d2l==1.0.0a1.post0) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->d2l==1.0.0a1.post0) (0.18.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/vincent/anaconda3/lib/python3.9/site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (0.13.1)\n",
      "Requirement already satisfied: argon2-cffi in /home/vincent/anaconda3/lib/python3.9/site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/vincent/anaconda3/lib/python3.9/site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (2.11.3)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /home/vincent/anaconda3/lib/python3.9/site-packages (from notebook->jupyter->d2l==1.0.0a1.post0) (0.13.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/vincent/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook->jupyter->d2l==1.0.0a1.post0) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==1.0.0a1.post0) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycparser in /home/vincent/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==1.0.0a1.post0) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/vincent/anaconda3/lib/python3.9/site-packages (from jinja2->notebook->jupyter->d2l==1.0.0a1.post0) (2.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from matplotlib->d2l==1.0.0a1.post0) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from matplotlib->d2l==1.0.0a1.post0) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from matplotlib->d2l==1.0.0a1.post0) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from matplotlib->d2l==1.0.0a1.post0) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vincent/anaconda3/lib/python3.9/site-packages (from matplotlib->d2l==1.0.0a1.post0) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from matplotlib->d2l==1.0.0a1.post0) (9.0.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.8.4)\n",
      "Requirement already satisfied: testpath in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.5.0)\n",
      "Requirement already satisfied: bleach in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (4.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (4.11.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.1.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.4)\n",
      "Requirement already satisfied: defusedxml in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0a1.post0) (0.5.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/vincent/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter->d2l==1.0.0a1.post0) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /home/vincent/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->jupyter->d2l==1.0.0a1.post0) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from pandas->d2l==1.0.0a1.post0) (2021.3)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from qtconsole->jupyter->d2l==1.0.0a1.post0) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vincent/anaconda3/lib/python3.9/site-packages (from requests->d2l==1.0.0a1.post0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/vincent/anaconda3/lib/python3.9/site-packages (from requests->d2l==1.0.0a1.post0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vincent/anaconda3/lib/python3.9/site-packages (from requests->d2l==1.0.0a1.post0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vincent/anaconda3/lib/python3.9/site-packages (from requests->d2l==1.0.0a1.post0) (3.3)\n",
      "Requirement already satisfied: asttokens in /home/vincent/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (2.0.5)\n",
      "Requirement already satisfied: executing in /home/vincent/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in /home/vincent/anaconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->d2l==1.0.0a1.post0) (0.2.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827636 sha256=df36d502800788f8067867572d72425f7b495cfbce7e301ccc70f5484b4706e6\n",
      "  Stored in directory: /home/vincent/.cache/pip/wheels/af/2b/30/5e78b8b9599f2a2286a582b8da80594f654bf0e18d825a4405\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, gym, d2l\n",
      "Successfully installed d2l-1.0.0a1.post0 gym-0.26.2 gym-notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install d2l==1.0.0a1.post0\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l\n",
    "d2l.use_svg_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Chargement de la base MNIST depuis Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La première étape consiste à charger les données MNIST selon la documentation de keras https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(d2l.DataModule):  #@save\n",
    "    def __init__(self, batch_size=64, resize=(28, 28)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.train, self.val = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fashion-MNIST se compose d'images de 10 catégories, chacune représentée par 6 000 images dans l'ensemble de données d'entraînement et par 1 000 dans l'ensemble de données de test. Un jeu de données de test est utilisé pour évaluer les performances du modèle (il ne doit pas être utilisé pour la formation). Par conséquent, l'ensemble d'apprentissage et l'ensemble de test contiennent respectivement 60 000 et 10 000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = FashionMNIST(resize=(32, 32))\n",
    "len(data.train[0]), len(data.val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les images sont en niveaux de gris et mises à l'échelle pour $32 \\times 32$ pixels en résolution ci-dessus. Ceci est similaire à l'ensemble de données MNIST original qui se composait d'images (binaires) en noir et blanc. Notez, cependant, que la plupart des données d'image modernes qui ont 3 canaux (rouge, vert, bleu) et des images hyperspectrales qui peuvent avoir plus de 100 canaux (le capteur HyMap a 126 canaux). Par convention, nous stockons l'image en tant que tenseur $c \\times h \\times w$, où $c$ est le nombre de canaux de couleur, $h$ est la hauteur et $w$ est la largeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les catégories de Fashion-MNIST ont des noms compréhensibles par l'homme. La fonction pratique suivante effectue la conversion entre les étiquettes numériques et leurs noms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(FashionMNIST)  #@save\n",
    "def text_labels(self, indices):\n",
    "    \"\"\"Return text labels.\"\"\"\n",
    "    labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [labels[int(i)] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Lecture d'un Minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour nous faciliter la vie lors de la lecture des ensembles d'entraînement et de test, nous utilisons l'itérateur de données intégré plutôt que d'en créer un à partir de zéro. Rappelons qu'à chaque itération, un itérateur de données lit un mini-lot de données de taille batch_size. Nous mélangeons également au hasard les exemples pour l'itérateur de données d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(FashionMNIST)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    data = self.train if train else self.val\n",
    "    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,\n",
    "                            tf.cast(y, dtype='int32'))\n",
    "    resize_fn = lambda X, y: (tf.image.resize_with_pad(X, *self.resize), y)\n",
    "    shuffle_buf = len(data[0]) if train else 1\n",
    "    return tf.data.Dataset.from_tensor_slices(process(*data)).batch(\n",
    "        self.batch_size).map(resize_fn).shuffle(shuffle_buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour voir comment cela fonctionne, chargeons un mini-lot d'images en appelant la *train_dataloader* méthode nouvellement ajoutée. Il contient 64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print(X.shape, X.dtype, y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regardons le temps qu'il faut pour lire les images. Même s'il s'agit d'un chargeur intégré, il n'est pas extrêmement rapide. Néanmoins, cela est suffisant car le traitement des images avec un réseau profond prend un peu plus de temps. Par conséquent, il est suffisant que la formation d'un réseau ne soit pas contrainte par les E/S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "for X, y in data.train_dataloader():\n",
    "    continue\n",
    "f'{time.time() - tic:.2f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous utiliserons assez fréquemment l'ensemble de données Fashion-MNIST. Une fonction de commodité show_imagespermet de visualiser les images et les étiquettes associées. Les détails de sa mise en œuvre sont reportés en annexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Faisons-en bon usage. En général, c'est une bonne idée de visualiser et d'inspecter les données sur lesquelles vous vous entraînez. Les humains sont très doués pour repérer les aspects inhabituels et, à ce titre, la visualisation sert de protection supplémentaire contre les erreurs et les erreurs dans la conception des expériences. Voici les images et leurs étiquettes correspondantes (en texte) pour les premiers exemples de l'ensemble de données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"@d2l.add_to_class(FashionMNIST)  #@save\n",
    "def visualize(self, batch, nrows=1, ncols=8, labels=[]):\n",
    "    X, y = batch\n",
    "    if not labels:\n",
    "        labels = self.text_labels(y)\n",
    "    d2l.show_images(tf.squeeze(X), nrows, ncols, titles=labels)\n",
    "\n",
    "batch = next(iter(data.val_dataloader()))\n",
    "data.visualize(batch)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous avons maintenant un ensemble de données légèrement plus réaliste à utiliser pour la classification. Fashion-MNIST est un ensemble de données de classification de vêtements composé d'images représentant 10 catégories. Nous utiliserons cet ensemble de données dans les sections et chapitres suivants pour évaluer diverses conceptions de réseau, d'un modèle linéaire simple à des réseaux résiduels avancés. Comme nous le faisons couramment avec les images, nous les lisons comme un tenseur de forme (taille du lot, nombre de canaux, hauteur, largeur). Pour l'instant, nous n'avons qu'un seul canal car les images sont en niveaux de gris (la visualisation ci-dessus utilise une fausse palette de couleurs pour une meilleure visibilité).\n",
    "\n",
    "- Enfin, les itérateurs de données sont un élément clé pour des performances efficaces. Par exemple, nous pourrions utiliser des GPU pour une décompression d'image efficace, un transcodage vidéo ou d'autres prétraitements. Dans la mesure du possible, vous devez vous fier à des itérateurs de données bien implémentés qui exploitent le calcul haute performance pour éviter de ralentir votre boucle de formation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Le Modèle de classification de base "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.  Le modèle de classification de base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous définissons la Classifierclasse ci-dessous. Dans le validation_step, nous rapportons à la fois la valeur de perte et la précision de la classification sur un lot de validation. Nous dessinons une mise à jour pour chaque num_val_batches lot. Cela a l'avantage de générer la perte moyenne et la précision sur l'ensemble des données de validation. Ces nombres moyens ne sont pas exactement corrects si le dernier lot contient moins d'exemples, mais nous ignorons cette différence mineure pour garder le code simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(d2l.Module):  #@save\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Par défaut, nous utilisons un optimiseur de descente de gradient stochastique (SGD), fonctionnant sur des mini-lots, tout comme nous l'avons fait dans le cadre de la régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Module)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return tf.keras.optimizers.SGD(self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Précision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compte tenu de la distribution de probabilité prédite `y_hat`, nous choisissons généralement la classe avec la probabilité prédite la plus élevée chaque fois que nous devons produire une prédiction dure. En effet, de nombreuses applications nécessitent que l'on fasse un choix. Par exemple, Gmail doit classer un e-mail dans \"Primaire\", \"Social\", \"Mises à jour\", \"Forums\" ou \"Spam\". Il peut estimer les probabilités en interne, mais en fin de compte, il doit en choisir une parmi les classes.\n",
    "\n",
    "- Lorsque les prédictions sont cohérentes avec la classe d'étiquettes `y`, elles sont correctes. La précision de la classification est la fraction de toutes les prédictions qui sont correctes. Bien qu'il puisse être difficile d'optimiser directement la précision (elle n'est pas différentiable), c'est souvent la mesure de performance qui nous importe le plus. C'est souvent la quantité pertinente dans les benchmarks. En tant que tel, nous le signalerons presque toujours lors de la formation des classificateurs.\n",
    "\n",
    "- La précision est calculée comme suit. Premièrement, si `y_hat` est une matrice, nous supposons que la deuxième dimension stocke les scores de prédiction pour chaque classe. Nous utilisons `argmax` pour obtenir la classe prédite par l'indice pour la plus grande entrée de chaque ligne. Ensuite, nous [**comparons la classe prédite avec la vérité terrain `y` élément par élément.**] . Étant donné que l'opérateur d'égalité `==` est sensible aux types de données, nous convertissons `y_hat` le type de données de pour qu'il corresponde à celui de `y`. Le résultat est un tenseur contenant les entrées 0 (faux) et 1 (vrai). La somme donne le nombre de prédictions correctes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(Classifier)  #@save\n",
    "def accuracy(self, Y_hat, Y, averaged=True):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    Y_hat = tf.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
    "    preds = tf.cast(tf.argmax(Y_hat, axis=1), Y.dtype)\n",
    "    compare = tf.cast(preds == tf.reshape(Y, -1), tf.float32)\n",
    "    return tf.reduce_mean(compare) if averaged else compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La classification est un problème suffisamment courant pour justifier ses propres fonctions de commodité. La précision du classifieur est d'une importance capitale dans la classification. Notez que même si nous nous soucions souvent principalement de la précision, nous formons des classifieurs pour optimiser une variété d'autres objectifs pour des raisons statistiques et informatiques. Cependant, quelle que soit la fonction de perte qui a été minimisée pendant la formation, il est utile de disposer d'une méthode pratique pour évaluer empiriquement la précision de notre classificateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "98684d9ca6d2941e865bd479f4bb4e1837c1fb374a896d53af5d1ba7f0e55dde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
