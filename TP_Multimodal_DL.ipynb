{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nYl1xE2GUxE"
   },
   "source": [
    "# **Multimodal Document Image Classification**\n",
    "# The models will be configured/trained using Tensorflow/Keras\n",
    "\n",
    "## **I- Visual Modality**\n",
    "\n",
    "In this part, we will train a visual model pretrained on ImageNet dataset (which is a large-scale dataset consisting of 1M images of 1000 classes). The aim of this first part is to extract the visual features of document images to perform document image classification on the Tobacco-3482 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRcvp461OY__"
   },
   "source": [
    "### A - Load Tobacco-3482 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAiriUNIOVo9"
   },
   "source": [
    "The Tobacco-3482 dataset is a document dataset which consists of 10 classes, and 3482 images will be found in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-76ZaZzlW1r"
   },
   "source": [
    "## **B - Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.3)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.37.4-py3-none-any.whl (960 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/vincent/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vincent/.local/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.0.5 cycler-0.11.0 fonttools-4.37.4 kiwisolver-1.4.4 matplotlib-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rm6Oxn8GUwZZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 14:16:40.147682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-07 14:16:47.923669: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/vincent/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-10-07 14:16:47.923724: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-07 14:16:49.674341: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-07 14:17:04.413773: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/vincent/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-10-07 14:17:04.414843: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/vincent/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-10-07 14:17:04.414890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD, Adam\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelBinarizer\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VnsEHefL6qR"
   },
   "source": [
    "# **C - Configure GPUs and Enable Distributed Strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GF2DrtHc0K4D"
   },
   "outputs": [],
   "source": [
    "# Memory growth must be set before GPUs have been initialized (if there are ones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erC16b1aMvy3"
   },
   "source": [
    "Enable GPU Memory Growth to not consume the whole memory while loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8E_zmFa0Trb"
   },
   "outputs": [],
   "source": [
    "#Configure CPU/GPU Distributed Training Strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97UPeKkQ0Zqo"
   },
   "source": [
    "Parameters to consider while building your visual *model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3hyhLZA6Sqt"
   },
   "outputs": [],
   "source": [
    "image_path = \"/content/tobacco/Image/\"\n",
    "text_path = \"/content/tobacco/Text/\"\n",
    "batch_size = 16\n",
    "output_classes = 10\n",
    "image_size = (224, 224)\n",
    "channels = 3\n",
    "dropout_rate = 0.1\n",
    "project_dim = 128\n",
    "EPOCHS = 10\n",
    "buffer_size = 3482"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca-zxUaFOAG-"
   },
   "source": [
    "# **G - Preprocess Inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1.   Create a function that processes the images as below\n",
    "    *   Read image\n",
    "    *   Resize image to image_size parameter\n",
    "    *   Normalize image so that our input is in the range of [0, 1]\n",
    "2.   Create a function that returns the class of the image given its path \n",
    "3.   Create a function that returns two lists: a list of image paths, and a list of labels (classes)\n",
    "4.   Create a function that:\n",
    "    *   shuffles data, with seed equal to the number of samples\n",
    "    *   converts target classes to categorical ones\n",
    "    *   Splits data to #Train, #Valid, and #Test sets, with ratios of 0.8, 0.1, 0.1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlyLt3N86eFA"
   },
   "outputs": [],
   "source": [
    "def Preprocess_Image(image):\n",
    "    # Read image using opencv\n",
    "\n",
    "    #Resize image using tf.image functions\n",
    "\n",
    "    #Normalize image in the range of [0, 1]\n",
    "\n",
    "    return image\n",
    "\n",
    "def getClass(image):\n",
    "    return # Return image class based on list entry (path) (e.g, input=\"/content/tobacco/Image/Scientific/image.jpg\" ==> output= \"Scientific\")\n",
    "    \n",
    "\n",
    "def prepare_image_data(data_path):\n",
    "\n",
    "    return images , labels #(e.g. images=['/content/tobacco/Image/Scientific/image1.jpg', /content/tobacco/Image/ADVE/image2.jpg, ...], labels=['Scientific', 'ADVE'])\n",
    "                  \n",
    "def create_image_split_dataset(data_path):\n",
    "    # Load Image Data and Labels from prepare_data function\n",
    "    images, labels = prepare_image_data(data_path)\n",
    "\n",
    "    # Shuffle images and Labels   using sklearn.shuffle   \n",
    "    \n",
    "    # Convert Categories to Binary labels using (from sklearn.preprocessing import LabelBinarizer)\n",
    " \n",
    "    # Split images into training, validation, and test \n",
    "\n",
    "    return data_train, labels_train, data_valid, labels_valid, data_test, labels_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV0KoR_RkKd7"
   },
   "source": [
    "Load Train, Valid, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bL1S9GpO6eMV"
   },
   "outputs": [],
   "source": [
    "# data_train, labels_train, data_valid, labels_valid, data_test, labels_test = create_image_split_dataset(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKLzugNN3aPI"
   },
   "source": [
    "# **Data Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6goF5PL3iDo"
   },
   "source": [
    "Visualize the repartition of data across the classes of Tobacco dataset: (Number of samples in each class) ==> Use Pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3vyBooC3h5A"
   },
   "outputs": [],
   "source": [
    "#Visualize data repartition across classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkshyKxc3ANJ"
   },
   "source": [
    "Visualize Some samples of the training data with matplotlib library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFXXZ2bV_RRu"
   },
   "outputs": [],
   "source": [
    "#Visualize some samples of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHS6n0S3si8"
   },
   "source": [
    "# **Prepare Data for Training**\n",
    "# Create a Data Loader to Load Data from Disk.\n",
    "The use of Data Generators is preferrable while dealing with huge amount of data. There are two methods that can be used to load data:\n",
    "\n",
    "*   \"tf.data.Dataset.from_tensor_slices\" for small amount of data\n",
    "*   \"tf.data.Dataset.from_generator\" for large amount of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeyXg08RBxwW"
   },
   "source": [
    "Create a Training, Validation, and Test Data Generators to load data in batches instead of saving the entire data **in** memory. The data generator yields images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95sZytie5R5T"
   },
   "outputs": [],
   "source": [
    "#example:\n",
    "def train_batch_generator():\n",
    "    while True:\n",
    "        #Loop through images and labels\n",
    "        #use the Preprocess_Image function to read and process images\n",
    "        yield train_images, train_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvEU31H2BXz_"
   },
   "outputs": [],
   "source": [
    "#Use tf.data.Dataset.from_generator() to load data in batches from the example above (train_batch_generator will be the input to tf.data.Dataset.from_generator)\n",
    "# Batch data to be loaded in batches of batch size parameter\n",
    "\n",
    "# Disable AutoShard.\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(...).batch(...).with_options(options)\n",
    "valid_dataset = tf.data.Dataset.from_generator(...).batch(...).with_options(options)\n",
    "test_dataset = tf.data.Dataset.from_generator(...).batch(...).with_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6FWkSiSqtoD"
   },
   "source": [
    "## **H - Load a Deep CNNs model to train our document image classification model (e.g. NASNetMobile)**\n",
    "\n",
    "## We will train our model using Transfer learning from pre-trained Imagenet weights\n",
    "\n",
    "Here we initialize the model with pre-trained ImageNet weights, and we fine-tune it on our own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qR39clpRXLC0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import NASNetMobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXaMAtqVdpsZ"
   },
   "outputs": [],
   "source": [
    "def visual_model():\n",
    "\n",
    "    #Initialize your model with a Input layer\n",
    "\n",
    "    #Load Nasnet_Mobile Model with imagenet weights and set include_top to False and set the number of classes\n",
    "\n",
    "    #Add a Globalaveragepooling2D layer\n",
    "\n",
    "    #Add a dropout layer with dropout rate equal to 0.1\n",
    "\n",
    "    #Add a dense layer with 128 units (project_dim parameter), and set \"relu\" as the activation function \n",
    "\n",
    "    #Add a dropout layer with dropout rate equal to 0.1\n",
    "\n",
    "    #Add a Dense layer with 10 units (each unit corresponds to a class), and an activation function=\"softmax\" to perform classification\n",
    "\n",
    "    #Define your functional with: tf.keras.Model()\n",
    "    model = tf.keras.Model(inputs = , outputs = )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgVCtOJMXLFS"
   },
   "outputs": [],
   "source": [
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "\n",
    "    #Load Model\n",
    "    model = visual_model()\n",
    "\n",
    "    # Freeze the pretrained weights from NasNet model, we won't train the whole model. \n",
    "    # We want to train and update the weights of only the added last layers\n",
    "\n",
    "    #compile your model using :\n",
    "      #Stochastic gradient Descent (SGD) as optimizer (set the learning rate to 1e-3, and momentum=0.9)\n",
    "      #Use an appropriate loss function\n",
    "      #Use an appropriate metric\n",
    "    freezed_model.compile(optimizer = , \n",
    "                  loss = , \n",
    "                  metrics = ,\n",
    "    )\n",
    "#model summary\n",
    "freezed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZDYpg6ue8V5"
   },
   "source": [
    "# ***I - Training***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbAl5deefJaZ"
   },
   "source": [
    "Note: the accuracy will increase very slowly and may overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV3cd-X6D2Du"
   },
   "source": [
    "Use model.fit to train your model. Define all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6GWMUYlXLIR"
   },
   "outputs": [],
   "source": [
    "# Train the model on all available devices.\n",
    "t0=time.time()\n",
    "\n",
    "freezed_history = freezed_model.fit()\n",
    "\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8280Xwpg00x"
   },
   "source": [
    "Evaluate the performance of your model on the Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN14X_1iFMLq"
   },
   "source": [
    "Use model.evaluate to evaluate the performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYB2V-RMglvN"
   },
   "outputs": [],
   "source": [
    "# Test the model after training\n",
    "scores = freezed_model.evaluate()\n",
    "\n",
    "#print loss and test accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGST5eCcfT6C"
   },
   "source": [
    "Plot models' history (training accuracy, validation accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xu_Cv-mwYKKZ"
   },
   "outputs": [],
   "source": [
    "#Create a function that plots the models' training accuracy, validation accuracy, training loss, and validation loss\n",
    "\n",
    "def plot_hist(hist):\n",
    "    \n",
    "plot_hist(freezed_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN1sVsvxiR3I"
   },
   "source": [
    "Re-Train the model and update all parameters. (Unfreez the first 2 layers).\n",
    "\n",
    "*   Re-compile your second model\n",
    "*   Re-Train your second model\n",
    "*   Compare the training time between the two trained models\n",
    "*   Compare Test accuracy between the two trained models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5V8hwFjLep"
   },
   "outputs": [],
   "source": [
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "\n",
    "    #Load Model\n",
    "    unfreezed_model = visual_model()\n",
    "\n",
    "    # Freeze the pretrained weights from NasNet model, we won't train the whole model. \n",
    "    # We want to train and update the weights of only the added last layers\n",
    "    \n",
    "\n",
    "    #compile your model using :\n",
    "      #Stochastic gradient Descent (SGD) as optimizer (set the learning rate to 1e-3, and momentum=0.9)\n",
    "      #Use an appropriate loss function\n",
    "      #Use an appropriate metric\n",
    "    unfreezed_model.compile(optimizer = , \n",
    "                  loss = , \n",
    "                  metrics = ,\n",
    "    )\n",
    "#model summary\n",
    "unfreezed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hc6syrewiRUs"
   },
   "outputs": [],
   "source": [
    "#Re-train the model and unfreez the first two layers\n",
    "# Train the model on all available devices.\n",
    "t1=time.time()\n",
    "\n",
    "unfreezed_history = unfreezed_model.fit()\n",
    "\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLSHSXfujw6M"
   },
   "outputs": [],
   "source": [
    "plot_hist(freezed_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoW1ik-Sg8ix"
   },
   "source": [
    "\\Create a directory (\"weights\") to save the weights of your trained model in the .h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tggn48omemPC"
   },
   "outputs": [],
   "source": [
    "!mkdir weights\n",
    "model.save_weights(filepath=\"/content/weights/visual_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FY3mS_6G50E"
   },
   "source": [
    "------\n",
    "------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enDuUMOiiF4z"
   },
   "source": [
    "## **II- Textual Modality**\n",
    "\n",
    "In this part, we will train a textual model. The aim of this second part is to extract the textual features of document images to perform document text classification on the Tobacco-3482 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXhWUxZEkD46"
   },
   "source": [
    "Load Train, Valid, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fy2ogxHbpb7W"
   },
   "outputs": [],
   "source": [
    "def getClass(text):\n",
    "    #return category (e.g, input=\"/content/tobacco/Text/Scientific/text.txt\" ==> output= \"Scientific\")\n",
    "    return\n",
    "\n",
    "def prepare_text_data(data_path):\n",
    "\n",
    "    return texts , labels #(e.g. images=['/content/tobacco/Text/Scientific/text1.txt', /content/tobacco/Text/ADVE/text2.jpg, ...], labels=['Scientific', 'ADVE'])\n",
    "                  \n",
    "def create_image_split_dataset(data_path):\n",
    "    # Load Text Data and Labels from prepare_text_data function\n",
    "    texts, labels = prepare_text_data(data_path)\n",
    "    # Shuffle texts and Labels using sklearn.shuffle  (use random_state parameter)  \n",
    "\n",
    "    # Convert labels to categorical (use from tensorflow.keras.utils import to_categorical) \n",
    "\n",
    "    # Split text data into training, validation, and test \n",
    "\n",
    "    return data_train, labels_train, data_valid, labels_valid, data_test, labels_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv9ZzjIRiAQr"
   },
   "outputs": [],
   "source": [
    "text_data_train, text_labels_train, text_data_valid, text_labels_valid, text_data_test, text_labels_test = create_split_dataset(text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eyv6_HNlkPRh"
   },
   "source": [
    "Import Libraries to process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wBbp3QwiAS8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP0wt5fOkOdC"
   },
   "source": [
    "Parameters to consider while building your *model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_0clD2u2dZH"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "buffer_size = 3482"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b3uh90tkg4p"
   },
   "source": [
    "Create a function that do the same thing as the function (prepare_image_data()), but instead of appending to the new list the list of text file paths, we will append the text readed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHJ7D6ci-w9e"
   },
   "outputs": [],
   "source": [
    "def feed_text(data, label):\n",
    "    text_data = []\n",
    "    text_labels = []\n",
    "\n",
    "    # Loop over your text data and labels\n",
    "\n",
    "    # Read the text of each file (.txt)\n",
    "\n",
    "    #Append both the text readed and text labels to text_data and to text_labels lists respectively  \n",
    "\n",
    "    return data, text_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDGfvN4dqgUo"
   },
   "source": [
    "Read your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5BhrDQ5B8OS"
   },
   "outputs": [],
   "source": [
    "text_data_train, text_labels_train = feed_text(...)\n",
    "text_data_valid, text_labels_valid = feed_text(...)\n",
    "text_data_test, text_labels_test = feed_text(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PiNQnQ-qpIP"
   },
   "source": [
    "Now, instead of using a data generator with \"tf.data.Dataset.from_generator\", we will be using a data loader with \"tf.data.Dataset.from_tensor_slices\" as we won't be dealing with images but texts, and they don't consume memory.\n",
    "\n",
    "It is a good chance for you to learn the two methods to preparing and loading data.\n",
    "\n",
    "Note: Use the parameters of \"tf.data.Dataset.from_tensor_slices(...)\" (e.g. .batch(), .prefetch(), .shuffle()). Check more information in tensorflow/keras websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9SkrhPd-ELr"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(...).batch(...)\n",
    "#Add parameters\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(...).batch(...)\n",
    "#Add parameters\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(...).batch(...)\n",
    "#Add parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIJGVsz7tjhE"
   },
   "outputs": [],
   "source": [
    "# The raw text loaded by the function feed_text() needs to be processed before it can be used in a model. \n",
    "# The simplest way to process text for training is using the \"tf.keras.layers.TextVectorization\" laye.\n",
    "# Create the TextVectorization layer. (use the max_tokens parameter)\n",
    "encoder = tf.keras.layers.TextVectorization(...)\n",
    "\n",
    "# Use the .adapt() method to set the layer's vocabulary\n",
    "# After the padding and unknown tokens they're sorted by frequency:\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qjjnrBUuVER"
   },
   "outputs": [],
   "source": [
    "# Here are the first 20 tokens.\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uc2uRNzpytVQ"
   },
   "outputs": [],
   "source": [
    "# Initially this returns a batch of the dataset of (text, label pairs):\n",
    "for example, label in train_dataset.take(1):\n",
    "    print('text: ', example.numpy())\n",
    "    print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQDh1_j1vIEi"
   },
   "source": [
    "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed output_sequence_length):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMPRho2pvGys"
   },
   "outputs": [],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPboKkiczmif"
   },
   "source": [
    "Now, let's define Our RNN model to perform text classification. We have defined the Visual model usinf a Functional Model (tf.keras.Model()). For the Textual Model, we will define  Sequential Model using tf.keras.Sequential() model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAFvrxiiiAfl"
   },
   "outputs": [],
   "source": [
    "def textual_model():\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "      #Add th encoder layer                           \n",
    "      encoder,\n",
    "      # Add an Embedding Layer(with input_dim == len(encoder.get_vocabulary(), and output_dim == 512 units)\n",
    "      \n",
    "      # Add a Bidirectional Layer(with layer as tf.keras.layers.LSTM layer with 512 units, activation_function=\"tanh\", and return_sequences == True))\n",
    "      \n",
    "      # Add a Bidirectional Layer(with layer as tf.keras.layers.LSTM layer with 256 units, activation_function=\"tanh\", and return_sequences == False))\n",
    "      \n",
    "      # Add a Dense Layer(with 128 units, and relu as activation function)\n",
    "      \n",
    "      # Add a Dropout Layer (with a dropout rate of 0.5)\n",
    "      \n",
    "      # Add a last dense Layer (with 10 units, which corresponds to the number of classes, and a \"softmax\" activation function)\n",
    "      ])\n",
    "    return model\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "\n",
    "  #Re-define the TextVectorization Layer and the .adapt() in the strategy scope \n",
    "  encoder = ...\n",
    "  encoder.adapt(...) \n",
    "\n",
    "  print('Build model...')\n",
    "  \n",
    "  text_model = textual_model()\n",
    "  #compile your model using :\n",
    "      #Adam optimizer (set the learning rate to 1e-4)\n",
    "      #Use an appropriate loss function\n",
    "      #Use an appropriate metric\n",
    "  text_model.compile(optimizer = , \n",
    "                     loss = , \n",
    "                     metrics = ,\n",
    "    )\n",
    "\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CA9cdskiAhQ"
   },
   "outputs": [],
   "source": [
    "t3=time.time()\n",
    "\n",
    "# Train the model on all available devices.\n",
    "text_history = text_model.fit(...)\n",
    "\n",
    "print('')\n",
    "print('total computing time: '+str(time.time()-t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEoncIp4G3q1"
   },
   "outputs": [],
   "source": [
    "# Test the model after training\n",
    "text_scores = text_model.evaluate()\n",
    "\n",
    "#print loss and test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWyAxFbIG3tG"
   },
   "outputs": [],
   "source": [
    "plot_hist(text_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ss0HTi4eG3vW"
   },
   "outputs": [],
   "source": [
    "model.save_weights(filepath=\"/content/weights/textual_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0486nk2FHNl1"
   },
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgqmkOmL2aig"
   },
   "source": [
    "## **III- Fusion Modality (Multimodal Modality)**\n",
    "\n",
    "In this part, we will load our pretrained visual and textual models. The aim of this second part is to get the extracted visual and textual features across the visual and textual modalities; Then, we will use two fusion strategies (early, and late fusion), to perform multimodal document image classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R4W8XDu3Cee"
   },
   "source": [
    "# ***A - Late Fusion***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGK9fDSZ3HHm"
   },
   "source": [
    "## 1. Load the Visual and Textual Models with their trained weights inside the strategy scope, and Define your final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-IZzxkhG30v"
   },
   "outputs": [],
   "source": [
    "# Open a strategy scope.\n",
    "\n",
    "#---- \n",
    "# Load Visual Model\n",
    "# Load the weights saved in the \"weights\" directory\n",
    "# Get the output(last layer) of the visual model\n",
    "\n",
    "#---- \n",
    "# Load Textual Model\n",
    "# Load the weights saved in the \"weights\" directory\n",
    "# Get the output(last layer) of the textual model\n",
    "\n",
    "#---- \n",
    "# Concatenate the outputs of the visual and textual models\n",
    "# Add a Dense layer on top of the concaenated outputs (with 20 units, and a \"relu\" activation function)\n",
    "# Add a Dense layer (wich corresponds to the last layer (with 10 units (number of classes), and a \"softmax\" activation function))\n",
    "\n",
    "#----\n",
    "# Finally, define your Functional Model (your inputs should be the image and text, and the output should be the last dense layer added to the model)\n",
    "\n",
    "#----\n",
    "# Compile your model:\n",
    "    #Take into account that now you have two inputs instead of only one (both image and text data)\n",
    "    #Use SGD optimizer, set the learning rate to 1e-3, and momentum to 0.9\n",
    "    #Use an appropriate loss function\n",
    "    #Use an appropriate metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T-mLREI3lDO"
   },
   "source": [
    "### 2. Modify the Data Loader to take into account a list of two inputs, instead of one.\n",
    "NB !! USE a DATA GENERATOR in the format of (tf.data.Dataset.from_generator()) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3gkbNBy4IVO"
   },
   "outputs": [],
   "source": [
    "#Modify Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nIF5hSQ7iyQ"
   },
   "source": [
    "### 2. Your Model is by now ready for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE0L9L-b7rSX"
   },
   "outputs": [],
   "source": [
    "#Train your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqdXyjfn8Bn4"
   },
   "source": [
    "### 3. Your Model is by now ready for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQtlSnMn7yMo"
   },
   "outputs": [],
   "source": [
    "#Evaluate your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-bffoxQ8Hgh"
   },
   "source": [
    "###Compare the Test accuracies of Visual, Textual, and Fusion Modalities\n",
    "\n",
    "Is there any improvement of accuracy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mm_ETQY8TWP"
   },
   "source": [
    "# ***B - Early Fusion***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro9aaiAb8WY4"
   },
   "source": [
    "## 1. Repeat Step A.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXJovMPo9CWI"
   },
   "source": [
    "Here, we will compare between 2 early fusion methods using these layers: \"concatenate\", \"average\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mMdUzLG8eZ4"
   },
   "outputs": [],
   "source": [
    "# Open a strategy scope.\n",
    "\n",
    "#---- \n",
    "# Load Visual Model\n",
    "# Load the weights saved in the \"weights\" directory\n",
    "# Get the output of the layer with 128 units of the visual model\n",
    "\n",
    "#---- \n",
    "# Load Textual Model\n",
    "# Load the weights saved in the \"weights\" directory\n",
    "# Get the output of the layer with 128 units of the textual model\n",
    "\n",
    "#---- 1st Method: \"Add\"\n",
    "# Add the outputs of the visual and textual models using the \"Add\" layer\n",
    "# Add a Dense layer on top of the added outputs (with 128 units, and a \"relu\" activation function)\n",
    "# Add a Dropout Layer (with a dropout rate of 0.5)\n",
    "# Add a Dense layer (wich corresponds to the last layer (with 10 units (number of classes), and a \"softmax\" activation function))\n",
    "\n",
    "#----\n",
    "# Finally, define your Functional Model (your inputs should be the image and text, and the output should be the last dense layer added to the model)\n",
    "\n",
    "#----\n",
    "# Compile your model:\n",
    "    #Take into account that now you have two inputs instead of only one (both image and text data)\n",
    "    #Use SGD optimizer, set the learning rate to 1e-3, and momentum to 0.9\n",
    "    #Use an appropriate loss function\n",
    "    #Use an appropriate metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUIkrXsy9qTY"
   },
   "source": [
    "### 2. Your Model is by now ready for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2ZaGmfu9u5Q"
   },
   "outputs": [],
   "source": [
    "#Train your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CKczIp_9vdg"
   },
   "source": [
    "### 3. Your Model is by now ready for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sPxNhwy9vT_"
   },
   "outputs": [],
   "source": [
    "#Evaluate your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq2LhLaJ963X"
   },
   "source": [
    "###Compare the Test accuracies of Visual, Textual, and Fusion Modalities\n",
    "\n",
    "Is there any improvement of accuracy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZJtfnrH-EJo"
   },
   "source": [
    "# 4. Repeat the same exact steps. Modify only the method of fusion and see results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xizu4dzY-OdX"
   },
   "source": [
    "# 5. Save the weights of your fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnTlly6S96Zn"
   },
   "outputs": [],
   "source": [
    "model.save_weights(filepath=\"/content/weights/fusion_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Miltimodal Deep Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
